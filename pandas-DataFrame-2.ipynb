{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizaje\n",
    "\n",
    "- Use glob para crear listas de archivos\n",
    "- Escribir  bucles para realizar operaciones en  archivos\n",
    "- Escribir comprensiones de lista para realizar operaciones en muchos archivos\n",
    "- combinar DataFrames de pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas series\n",
    "\n",
    "Al igual que NumPy tiene su tipo de datos de objeto ndarray, Pandas tiene dos tipos de datos principales que le permiten realizar todas las cosas interesantes. Empezamos con Serie.\n",
    "\n",
    "Las series son básicamente matrices unidimensionales, que tienen un nombre y un nombre de índice. Los nombres pueden ser números enteros o cadenas, pero tienen que ser únicos. En pocas palabras, ¿recuerda los objetos de listas incorporados? Las listas consisten en diferentes valores que podrían ser llamados por un índice entero. Las series son casi iguales, pero también permiten el uso de nombres específicos en lugar de ordenar el índice. Veremos el ejemplo de esto un poco más tarde. Otra diferencia clave es que Series no permite tipos de datos mixtos dentro de un objeto.\n",
    "\n",
    "ndarray unidimensional con etiquetas de eje, que no necesita ser único. El objeto admite la indexación basada en enteros y etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18.0\n",
      "1    21.5\n",
      "2    21.0\n",
      "3    21.0\n",
      "4    18.8\n",
      "5    17.6\n",
      "6    20.9\n",
      "7    20.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temperature = [18.0, 21.5, 21.0, 21.0, 18.8, 17.6, 20.9, 20.0]\n",
    "temperature_series = pd.Series(temperature)\n",
    "print(temperature_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puedeb realizar operaciones numéricas de forma simple \"vectorizada\" sin recorrer todos los valores.\n",
    "\n",
    "- Las Series tienen sus propios métodos únicos que se les pueden aplicar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    64.40\n",
      "1    70.70\n",
      "2    69.80\n",
      "3    69.80\n",
      "4    65.84\n",
      "5    63.68\n",
      "6    69.62\n",
      "7    68.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "temperature_series = temperature_series * 9/5 + 32 # numerical operations\n",
    "print(temperature_series)\n",
    "print(temperature_series.mean()) # useful methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de los datos\n",
    "\n",
    "- Para filtrar los datos, debe especificar la condición para filtrar. De la misma forma que hicimos antes, puedes combinar múltiples condiciones con `&` (AND) o `|` (O) operaciones.\n",
    "\n",
    "- En el primer ejemplo, estamos tomando todas las observaciones que cumplen con los criterios: el sexo es femenino y la edad es mayor de 60 años. Tenga en cuenta que no tiene que crear un objeto separado (condición), se hizo solo para una mejor visualización. representación.\n",
    "\n",
    "- En el segundo ejemplo, primero tomamos el ID de la columna y luego filtramos la serie según la condición de que el volumen cerebral total normalizado (nWBV) debe estar en un rango entre 0,7 y 0,8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtre un DataFrame completo por condición específica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (dementia_df['M/F'] == 'F') & (dementia_df['Age'] > 60)\n",
    "dementia_df[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccione una columna y filtre por condición:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dementia_df['ID'][dementia_df['nWBV'].between(0.7, 0.8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra herramienta útil es la agrupación y agregación de los datos. En este ejemplo, queremos ver el volumen intracraneal total mínimo estimado (eTIV) y el volumen cerebral total normalizado promedio (nWBV) para cada sexo y clasificación de demencia clínica (CDR). Esta línea de código puede parecer un poco complicada, pero podemos dividirla en partes:\n",
    "\n",
    "1. Agrupe por una lista de nombres de columna. Si quisiera dividir solo por una columna, podría especificar solo una cadena con un nombre de columna, por ejemplo, por=\"CDR\".\n",
    "\n",
    "2. En este momento tenemos un objeto DataFrame \"agrupado\", que no es tan interesante. Aplicamos el método de agregación y especificamos un diccionario de la siguiente manera: {\"<nombre de columna>\": \"<función de agregación>\"}. Si desea aplicar varias funciones en la misma columna, puede especificar una lista, por ejemplo, {\"eTIV\": [\"min\", \"max\"]}.\n",
    "\n",
    "Clasificación de demencia clínica: 0 = sin demencia, 0,5 = EA muy leve, 1 = EA leve, 2 = EA moderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dementia_df.groupby(by=[\"CDR\", \"M/F\"]).agg({\"eTIV\": \"min\", \"nWBV\": \"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unir datos con pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si nuestros datos se encuentran en dfierentes tablas?. Puede unir todas las tablas para analizar todo a la vez.\n",
    "\n",
    "Hay tres formas principales de hacer esto:\n",
    "\n",
    "- Unir internamente\n",
    "- Unión izquierda o unión derecha (según la tabla que llame \"izquierda\" y \"derecha\")\n",
    "- Unión completa\n",
    "\n",
    "Tenga en cuenta que es importante que tenga una columna compartida para unir los datos.\n",
    "\n",
    " ![Tipos](C:/Users/STOCK-LAP403/OneDrive - Storecheck S.A. de C.V/Escritorio/Modelos/imagenesJoins.png)\n",
    "\n",
    "**Fuente :** [w3schools](https://www.w3schools.com/sql/sql_join.asp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join\n",
    "En el siguiente ejemplo tenemos dos tablas:\n",
    "\n",
    "- Tabla A: contiene identificaciones y nombres de sujetos\n",
    "- Tabla B: cédulas de titulares y ocupación de los sujetos\n",
    "Unimos los datos por el `Id` de la columna. ¿Qué asaría si no tuviéramos la columna `Id` en la tabla B?. ¿Podríamos unir estas dos tablas?\n",
    "\n",
    "Con la combinación interna, tomamos solo aquellas observaciones que tienen valores de `Id` coincidentes en ambas tablas (estas observaciones están marcadas con marcas verdes). Tenga en cuenta que hubo dos observaciones en la tabla B con `Id` \"1\", por lo que en una tabla resultante (abajo a la derecha) tenemos dos observaciones para \"Bob\".\n",
    "\n",
    " ![Tipos](Join_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Join\n",
    "\n",
    "Al realizar una combinación izquierda, debemos definir una tabla como \"izquierda\" y la otra como \"derecha\". En este ejemplo, la tabla A es \"izquierda\" y la tabla B es \"derecha\". Tomamos todas las observaciones de la tabla A y unimos las observaciones coincidentes de la tabla B (coincidentes con la columna `Id`).\n",
    "\n",
    "Algunas observaciones de la tabla A no tenían observaciones coincidentes en la tabla B, por eso vemos valores faltantes en una tabla.\n",
    "\n",
    "Ua idea de la unión derecha es básicamente la misma, pero en ese caso, tomaríamos todas las observaciones de la tabla de la derecha y agregaríamos las coincidencias de la tabla de la izquierda. Entonces, si hicimos una combinación derecha con la tabla A \"derecha\" y la tabla B \"izquierda\", terminaríamos con los mismos resultados.\n",
    "\n",
    " ![Tipos](Left_Join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al unir las tablas por unión completa, tomamos todas las observaciones de ambas tablas. En este ejemplo, tenemos observaciones que no tienen observaciones coincidentes, por eso vemos más valores faltantes.\n",
    "\n",
    "![Tipos](Full_Join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comandos de pandas para hacer Join\n",
    "Para realizar la unión en pandas podemos usar la función `pd.merge()`. Si la columna de unión tiene el mismo nombre en ambos DataFrame, puede usar el argumento. Si los nombres fueran diferentes (por ejemplo, `Id` para table1 y `subj_Id` para table2), entonces tendríamos que usar los argumentos left_on y right_on (`left_on=\"Id\"`, `right_on=\"subj_Id\"`).\n",
    "\n",
    "Los valores faltantes se representan como objetos np.NaN.\n",
    "\n",
    "Tenga en cuenta que Full Join se llama `\"outer\"` y en Pandas es (`how=\"outer\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bob</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jack</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jill</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ben</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Name Occupation\n",
       "0   1   Bob         IT\n",
       "1   1   Bob    Finance\n",
       "2   2  Jack        NaN\n",
       "3   3  Jill         IT\n",
       "4   4   Ben        NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tabla1 = pd.DataFrame(\n",
    "    {'Id': [1, 2, 3, 4],\n",
    "    'Name': ['Bob', 'Jack', 'Jill', 'Ben']})\n",
    "\n",
    "tabla2 = pd.DataFrame(\n",
    "    {'Id': [1,1,3,5,7,7],\n",
    "    'Occupation': ['IT', 'Finance', 'IT', 'Healthcare', 'Agriculture', 'Finance']})\n",
    "\n",
    "pd.merge(\n",
    "    left=tabla1, right=tabla2,\n",
    "    on='Id', # or left_on='Id', right_on='Id',\n",
    "    how='left')\n",
    "#ff =tabla1.merge(tabla2,on='Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de bucle `for `para procesar archivos dada una lista de sus nombres\n",
    "\n",
    "- Podemos usar un bucle `for` para leer un conjunto de archivos de datos y hacer algo para cada uno. En este caso, imprimiremos el valor mínimo en cada archivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_files = ['data/gapminder_gdp_africa.csv', 'data/gapminder_gdp_asia.csv']\n",
    "\n",
    "for filename in data_files:\n",
    "    data = pd.read_csv(filename, index_col='country')\n",
    "    print(filename, data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de `glob.glob` para encontrar conjuntos de archivos cuyos nombres coincidan con un patrón\n",
    "\n",
    "- En Unix, el término globbing significa hacer coincidir un conjunto de archivos con un patrón.\n",
    "\n",
    "- Los patrones más comunes son:\n",
    "\n",
    "    - `*` lo que significa que coincide con cero o más caracteres\n",
    "\n",
    "    - `?` el significado coincide exactamente con un carácter\n",
    "\n",
    "- La biblioteca estándar de Python contiene el módulo glob para proporcionar la funcionalidad de coincidencia de patrones\n",
    "\n",
    "- El módulo glob contiene [`glob`](https://docs.python.org/3/library/glob.html) una función también llamada glob para hacer coincidir patrones de archivos\n",
    "\n",
    "Por ejemplo, `glob.glob('*.txt')` coincide con todos los archivos del directorio actual cuyos nombres terminan en `.txt`.\n",
    "\n",
    "- El resultado es una lista de cadenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "print('all csv files in data directory:', glob.glob('data/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use glob y for para procesar por batch los archivos\n",
    "\n",
    "Es una buena práctica nombrar sus archivos sistemáticamente. Como ha aprendido, Python es muy preciso en cosas como el uso de mayúsculas, por lo que si los nombres de sus archivos son inconsistentes (por ejemplo, `Gapminder_Europe.csv, gapminder_americas.csv, gapminder_Oceania.csv`), entonces es más difícil escribir código con `glob` que funcione correctamente. .\n",
    "\n",
    "Para los datos de Gapminder, afortunadamente los nombres de los archivos son bastante sistemáticos y consistentes (al igual que los nombres de las columnas dentro de cada archivo), por lo que podemos usar lo siguiente para leer cada uno e imprimir el PIB mínimo desde 1952:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob('data/gapminder_gdp*.csv'):\n",
    "    data = pd.read_csv(filename)\n",
    "    print(filename, data['gdpPercap_1952'].min())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregar archivos a un solo Dataframe\n",
    "\n",
    "- A menudo, no solo queremos abrir un archivo y extraer una pequeña cantidad de datos (como el valor mínimo en los ejemplos anteriores). Más bien, podríamos querer abrir un conjunto de archivos de datos relacionados y combinarlos en un gran DataFrame. Por ejemplo, en psicología y neurociencia, la mayoría de los experimentos involucran a múltiples participantes. Para cada participante, cuando ejecutamos el experimento obtenemos un archivo de datos. Para analizar los datos entre los participantes, nos gustaría leer los archivos de datos de todos los participantes y combinarlos en un DataFrame.\n",
    "\n",
    "- pandas tiene algunos métodos que nos permiten combinar DataFrames, que incluyen:\n",
    "\n",
    "- [`.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "- [`.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html?highlight=merge#)\n",
    "- [`.append()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html?highlight=append#pandas.DataFrame.append)\n",
    "- [`.join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join)\n",
    "\n",
    "Nos centraremos aquí en el primero. `concat` significa \"concatenar\", que esencialmente significa combinar archivos \"apilándolos\". Es decir, comience con un DataFrame y agregue un nuevo DataFrame en la parte inferior del mismo, creando filas adicionales. Asumimos que todos los archivos de datos que estamos leyendo tienen las mismas columnas. Por ejemplo, en los conjuntos de datos de PIB de Gapminder, cada archivo tiene una columna para `country` más una serie de columnas para el PIB en diferentes años, y los mismos años están en las columnas de todos los conjuntos de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos de múltiples participantes experimentales\n",
    "\n",
    "Digamos que tenemos datos de un experimento en el que realizamos tres participantes humanos (a veces llamados \"personas\") en días diferentes. Para cada participante, tenemos un archivo de datos. Las columnas en todos los archivos son iguales, porque los archivos fueron generados por un programa de computadora que ejecutó el experimento.\n",
    "\n",
    "Damos a los participantes códigos de identificación anónimos para proteger su privacidad y permitir una convención de nomenclatura simple y sistemática para los archivos. los datos del primer participante se guardan en un archivo llamado s1.csv, los del segundo en s2.csv, etc.\n",
    "\n",
    "Podemos glob la carpeta de datos en la que se almacenan los archivos, para encontrar todos los archivos CSV cuyos nombres comienzan con una s seguida de un solo carácter, seguido de .csv. Guardaremos el resultado en una lista que podemos recorrer más tarde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('data/s?.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, creamos una lista vacía en la que almacenaremos los DataFrames de cada participante. Terminará siendo una lista de DataFrames (recuerde, las listas pueden contener casi cualquier otro tipo de datos de Python), y una vez que hayamos leído todos los datos, los combinaremos en un DataFrame. Este es un truco que es importante usar en pandas. La razón tiene que ver con cómo pandas combina DataFrames y los almacena en la memoria. En términos simples, cada vez que concatenamos DataFrames, pandas realiza muchas comprobaciones internas para asegurarse de que no haya errores. Hacer esta verificación una vez, al combinar muchos DataFrames, es mucho más eficiente (y por lo tanto más rápido) que hacerlo muchas veces. Del mismo modo, cuando se crea un DataFrame, se le asigna una cantidad adecuada de espacio de memoria en la computadora. Cada vez que agregamos datos adicionales, tenemos que crear un bloque de memoria nuevo y más grande. Asignar nuevos bloques de memoria, muchas veces, lleva más tiempo que hacerlo solo una vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, usamos un bucle for para leer los archivos. Esto recorrerá los elementos en la lista de nombres de archivo; cada vez que pasa por el ciclo, el nombre del archivo tiene el valor del nombre del archivo actual, y usamos el método list append() para agregar los datos de ese archivo a df_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    df_list.append(pd.read_csv(f))\n",
    "\n",
    "df_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer varios archivos usando la comprensión de listas\n",
    "\n",
    "Si bien el bucle for anterior funciona bien, hay una forma alternativa de hacerlo, utilizando [**comprensión de listas**](https://neuraldatascience.io/3/for-loops.html#list-comprehension). Recuerde que las listas por comprensión son básicamente una versión compacta de un ciclo `for`, pero tienen algunas ventajas:\n",
    "\n",
    "- son más pitónicos: solo requieren una línea de código, mientras que el bucle for anterior requiere dos\n",
    "\n",
    "- son más eficientes: las listas de comprensión en realidad se ejecutan más rápido. Esto puede no ser un problema en los pequeños ejemplos aquí, pero puede marcar una gran diferencia cuando se trabaja con grandes conjuntos de datos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [pd.read_csv(f) for f in filenames]\n",
    "df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinar DataFrames\n",
    "\n",
    "- En este punto, hemos leído cada archivo de entrada y lo hemos almacenado como un DataFrame, pero tenemos una lista de tres DataFrames distintos. En la mayoría de los casos, querremos combinarlos de alguna manera. Habiendo construido nuestra lista de DataFrames a través de la lectura de un conjunto de archivos, podemos combinarlos en un solo DataFrame usando el método pandas .concat():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)\n",
    "\n",
    "# Confirm this worked by viewing a random sample of rows\n",
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de la columna de índice\n",
    "Recuerde que las etiquetas de fila en pands se llaman índices. Podemos convertir cualquier columna en un índice usando el método `.set_index()`. Para estos datos, un índice apropiado es el del `participante`, que se encuentra en la columna Participante. Tenga en cuenta que debemos asignar el resultado de la operación `.set_index()` de nuevo a df para que se almacene el cambio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Participant')\n",
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Tiene dos DataFrames cargados. Uno tiene `ID` de pacientes y el estado del cáncer de mama (maligno o benigno). El otro tiene identificaciones de pacientes y algunas características para el núcleo celular.\n",
    "\n",
    "    - radio_medio: media de las distancias del centro a los puntos del perímetro;\n",
    "    - textura_media: desviación estándar de los valores en escala de grises;\n",
    "    - perímetro: tamaño medio del tumor central.\n",
    "    \n",
    "        ¿El valor promedio de radius_mean es mayor para el tipo maligno?\n",
    "\n",
    "       [read_jason](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html)\n",
    "\n",
    "        ~~~python\n",
    "        table_1 = pd.read_json(\"path/table1.json\")\n",
    "        table_2 = pd.read_json(\"path/table2.json\")\n",
    "\n",
    "        joined_table = pd.merge(left=___, ___=table_2, how=___,\n",
    "                                left_on=___, ___=___)\n",
    "\n",
    "        radius_mean_benign = joined_table[___]['radius_mean'].___()\n",
    "        radius_mean_malignant = ___\n",
    "\n",
    "        print(radius_mean_malignant > radius_mean_benign)\n",
    "        ~~~\n",
    "2. Obtenga todas las características del núcleo celular y etiquételos con el tipo de cáncer. Cuando no se especifique el tipo de cáncer, márquelo como \"desconocido\". No cambie los valores que faltan en otras columnas.\n",
    "\n",
    "    El DataFrame unido tendrá dos columnas `\"id\"` e `\"ID\"`. Mantenga sólo la primera columna.\n",
    "\n",
    "    Hint! Para reemplazar el valor que falta, puede usar el método `.fillna()`. Se puede aplicar en Series o DataFrame.\n",
    "    ~~~python\n",
    "    import pandas as pd\n",
    "\n",
    "    table_1 = pd.read_json(\"exercises/data/table1.json\")\n",
    "    table_2 = pd.read_json(\"exercises/data/table2.json\")\n",
    "\n",
    "    joined_table = ___  # join two tables together\n",
    "    joined_table.___(labels=___, ___=1, inplace=___) # drop the redundant column\n",
    "    ___                 # replace the missing values in a column\n",
    "\n",
    "    display(joined_table)\n",
    "    ~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "213a5d771efc885c0a0d146cb9398e98aa15d9474476d60ee972b98bffb9e082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
